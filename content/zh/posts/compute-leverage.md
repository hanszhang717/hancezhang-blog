---
title: "算力是新的杠杆（而且可能是最好的）"
date: 2025-08-08
draft: false
summary: "随着算力作为杠杆超越了人力，广泛共识已不再是必需品"
categories: ["AI"]
slug: "compute-leverage"
---

# **算力是新的杠杆（而且可能是最好的）**

纳瓦尔曾概述了一个杠杆的层级体系。最古老的是**人力**，也就是为你工作的人。它很强大，但也很混乱。管理人很难，因为让人们保持一致很难，而且沟通成本的增长速度超过了团队规模。在社会的大多数领域——政府、大公司，甚至社交圈——做一个“逆势且正确”的人并不会得到奖励，反而常常会给你带来麻烦。

但创业和投资的世界则不同。它们是少数几个“逆势且正确”不仅被容忍，而且是巨大回报来源的地方。你不需要达成广泛共识就能购买一支股票或创办一家公司。事实上，共识往往意味着机会已经消失。这之所以行得通，是因为这些领域已经开始超越人力，不再将其作为主要杠杆，而是依赖**资本**和**代码**。代码是一种极好的杠杆：编写一次，就可以被运行一百万次，边际成本几乎为零。它是无需许可且可扩展的。

这个根本性的洞见解释了为什么我总是在独自一人或在非常小的团队中才能做出最好的工作。达成“广泛共识”——那种在许多人之间脆弱又昂贵的协议状态——是对每一个新想法征收的税。多年来，我以为这只是我个人的怪癖，但后来我意识到，这是价值创造的核心原则。

随着技术再次改变主导的杠杆形式，广泛共识的重要性正在开始消退。我相信我们正处在一个新的、甚至更强大的杠杆时代的开端：**算力**。

在那些可以被自动化的信息密集型领域，算力正在成为比人力、代码或资本更高效、更可扩展的杠杆。AI 是解锁这一切的关键。它将编排海量计算能力的任务门槛从专家级的工程能力降低到了单个创造者就能做到的水平。这一转变将从根本上改变我们组建团队、设计组织和分配回报的方式。

### **从协调人到编排算力**

创造事物的核心任务正在改变。过去是找到合适的人并协调他们的努力方向，责权利等。现在，它越来越倾向于清晰地定义一个问题，并指挥计算资源去解决它。AI 允许我们将任务“外包”给模型和 Agent，而不是给其他人。现在，单一个体就可以将一个想法转化为产品、一项研究或一个创意作品，而所需的人际协调要少得多。

我们可以用一个简单的模型来描述这一点。把实现一个目标所需的总工作量（**Work**）看作一个常数。这个工作量是人数、他们能调动的算力以及一些外部因素的函数。

一个简化的公式可能如下所示：

**Work \= N × Cᵖ × f(Data, Clarity, Distribution)**

我们来分解一下：

* **N** 是人数（headcount）。  
* **C** 是人均算力（compute per person）——即单个个体能有效调动和管理的计算能力。  
* **p** 是算力的效率指数。如果 p \= 1，那么每一单位的算力都是完全高效的。现实中，p \< 1，因为存在编排的开销。  
* **f(...)** 是外部约束的乘数，比如优质数据的可得性、问题的清晰度以及最终产品的分发能力。

对于一个固定的 **Work**，如果 **C**（人均算力）上升，并且更好的工具使得管理算力变得更容易（将 **p** 推向 1），那么最优的 **N**（人数）就必然会下降。

这并非一个全新的现象。云计算和 CI/CD 管道的兴起已经在一个较小的尺度上证明了这个模型。但 AI 将这条曲线的斜率提升了一个数量级。

当然，这个模型也有其局限性。你可能会达到 **“编排饱和”**，即你拥有的算力超出了单一个人能有效管理的范围，导致你的效率（p）下降。或者，你可能受制于外部因素（f）。如果你没有好的数据或一个清晰的问题，全世界的算力也只是在原地空转。而如果你无法触达用户，你那由算力生成的绝妙产品也将无人问津。

让我们用一个具体的例子来说明。

### **AI 编程的 S 曲线**

看看软件开发领域。得益于 AI 编程助手，每个工程师可用的算力（**C**）正在稳步上升。随着这些工具变得越来越好，一个小团队——甚至一个开发者——就能实现过去需要一个更大团队才能完成的产出（**Work**）。所需的人数（**N**）正在下降。

我们已经可以看到指标在变化。想象一个团队的目标是每个季度发布 10 个新功能。在过去，一个八人团队可能会因为积压的代码审查（**PRs**）而耽搁数天。现在，一个四人团队，配备了强大的 AI 助手，可以自动生成样板代码、单元测试甚至重构建议，将审查时间缩短到几小时。他们同样发布了 10 个功能，但人数减半，协调成本也大大降低。

争论的焦点不在于 AI *是否* 提高了生产力，而在于认识到生产力的 *形态* 正在改变。它变得越来越不依赖于工程师的数量，而更多地依赖于支持每个工程师的算力层的厚度。

这催生了一种新的组织形式：**“薄团队 \+ 厚算力”**（thin core with thick compute）。核心团队很小，专注于战略和定义清晰的目标。执行则由一个由 AI Agent 和自动化工作流组成的广阔外围来处理。像 **Midjourney** 这样的公司就是一个完美的例子。它的核心团队极小，专注于研究、产品愿景和模型调优。它的“员工队伍”是一个为数百万用户服务的、可无限扩展的算力层。他们不雇佣成百上千的艺术家，他们编排 **GPU**。这就是“薄团队，厚算力”模式的实际应用。在这种模式下，做一个“逆势且正确”的人所获得的回报更加直接。你不需要说服一个委员会，你只需要说服市场，而市场奖励的是产品结果，而不是你是否追随大流。

### **算力会成为新的生产资料吗？**

如果算力是一种新的生产资料，我们必须追问它的分配问题。像土地一样，算力需要资本来获取（**GPU**、云服务点数、能源）。它具有竞争性，并且有规模经济效应。

这引出了“算力地主”的假说。少数几家云服务提供商和芯片制造商会成为寡头，向其他所有成为“算力佃户”的人收取租金吗？这是有可能的。

但也存在强大的反作用力。开源模型正在使算力的获取民主化。社区驱动的项目正在汇集 **GPU** 资源。国家的产业政策正在试图防止垄断。而且，随着新芯片的发布，更旧、性能较弱的芯片将涌入市场，使得一个基础水平的算力几乎人人可得。看看围绕 **Llama** 或 **Stable Diffusion** 等模型建立的开源社区。许多个人开发者正在消费级 **GPU**（比如 **RTX 4090**）上运行和微调这些模型。这就像数字时代的自给自足农业；虽然它无法与 **AWS** 或 **Google Cloud** 这样的工业级“算力地主”竞争，但它确保了生产资料不会被完全垄断。

算力价格的长期趋势是下降的，而获取渠道正在扩大，这在某种意义上遵循了Wright's Law。然而，尖端的高性能算力在一段时间内可能仍然是一种受限的资源，从而造成一种新的不平等。

### **这个模型的失效之处**

算力并非万能灵药。它只有在特定条件下才是最佳的杠杆。

当一个问题可以被形式化定义，数据充足，且分发是可解问题时，算力的效果最好。如果你的挑战是建立一个品牌、应对复杂的监管环境或管理一个实体供应链，那么其他形式的杠杆——比如资本、媒体，甚至老式的人力——可能仍然更优越。例如，考虑建一所新医院。你可以用算力来设计建筑和优化病人流程（处理“比特”）。但你仍然需要处理区划法规、获得许可、管理施工团队、并与当地社区建立信任。这些都是“人类”世界的问题，其中人际关系、信任和在官僚体系中周旋——这些传统的人力和社会资本杠杆，远比原始算力更重要。

大公司不会消失。它们仍然可以凭借其现有的护城河获胜：分发渠道、专有数据和合规机制。算力是一个加速器，而不是护城河本身。

此外，算力的价格并不能保证会平稳下降。能源成本、芯片短缺和地缘政治都可能抬高“C”的价格，从而减缓其采用的 S 曲线。